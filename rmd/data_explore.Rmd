---
title: "Fully Deployed Front to Back End Fantasy Projection Model in R"
author: "Duncan Gates"
navlink: "[More Data Stuff](http://duncangates.me/)"
og:
  type: "article"
  title: "opengraph title"
  url: "optional opengraph url"
  image: "optional opengraph image link"
footer:
  - content: '[Shiny Apps](http://shinyapps.duncangates.me/) • [Github](http://www.github.com/dungates/)<br/>'
  - content: 'Copyright © Duncan Gates, 2021'
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: DGThemes::skeleton
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, fig.height = 10, fig.width = 10)
library(tidyverse)
library(tidymodels)
library(DGThemes)
library(gt)
library(scales)
library(paletteer)
theme_set(theme_duncan())
df <- readr::read_csv(here::here("Data/player_season_totals.csv"))
```

## Introduction

#### Getting the Data

In order to predict the fantasy scores of NBA players in the 2021-2022 season I pulled data from basketball reference using `nbastatR`, grabbing the last 20 years of season totals from players, in both normal and advanced stats, which can conveniently be done with just one line of code!

```{r eval = F, echo = T}
df <- nbastatR::bref_players_stats(seasons = 2000:2021, tables = c("totals", "advanced"), widen = TRUE, assign_to_environment = F)
```

The only initial data manipulation is to create the response variable for the dataset, the fantasy points, which is done with a very simple formula in my league, the coefficients are arbitrarily chosen and lead to some interesting weighting in our modelling later on.

```{r eval = F, echo = T}
coefs <- tibble::tibble(Points = 1,
                        Rebounds = 1.2,
                        Assists = 1.5,
                        Steals = 3,
                        Blocks = 3,
                        Turnovers = -1,
                        `3 Point FGM` = 1) %>%
  pivot_longer(everything(), names_to = "Statistic", values_to = "Multiplier") %>%
  gt::gt() %>%
  data_color(
    columns = c(Multiplier),
    colors = scales::col_numeric(
      palette = paletteer::paletteer_d(
        palette = "rcartocolor::Mint"
      ) %>% as.character(),
      domain = NULL
    )
  ) %>%
  DGThemes::gt_theme_duncan()
coefs %>%
  tab_header(title = "Multipliers for Points in my Fantasy League")
```

#### Setting up the Database

The data we have is tabular, and can easily be setup an id which I did by adding a column that is just the season and player name pasted together (with additional notation of 01 to ensure players with the same name don't get merged) in the format that is slightly more graphically represented below with the top 10 fantasy seasons ever in my league.

```{r eval = T, echo = F}
df %>%
  mutate(id = slugPlayerSeason) %>%
  arrange(desc(fantasy_points)) %>%
  select(id, fantasy_points, url_player_headshot = urlPlayerHeadshot) %>%
  slice(1:10) %>%
  gt::gt() %>%
  data_color(
    columns = c(fantasy_points),
    colors = scales::col_numeric(
      palette = paletteer::paletteer_d(
        palette = "ggsci::red_material"
      ) %>% as.character(),
      domain = NULL
    )
  ) %>%
  text_transform(
    locations = cells_body(columns = url_player_headshot),
    fn = function(x) {
      web_image(
        url = x,
        height = as.numeric(50)
      )
    }
  ) %>%
  DGThemes::gt_theme_duncan()
```

This data lends itself quite easily to an SQLite relational database which I setup with `RSQLite`. First I save the data as a csv in the data folder, and then I establish a local connection and write the table to the database file. In this case I set `overwrite = TRUE` since I have written to the db before and am just saving it again, but it is generally unnecessary.

```{r eval = F}
readr::write_csv(df, "Data/player_season_totals.csv")

con <- DBI::dbConnect(RSQLite::SQLite(), dbname = "nba.db")

upload_data <- readr::read_csv("Data/player_season_totals.csv")

RSQLite::dbWriteTable(conn = con, 
                      name = "player_seasons", 
                      value = upload_data, 
                      overwrite = TRUE)
```

#### Regular Updates

Now to add data to the table regularly given the oncoming 2021-2022 season I add to the table regularly by running the following script on the server I hosted the data on.

```{r eval = F}
new_df <- nbastatR::bref_players_stats(2022, tables = c("totals", "advanced"), widen = T, assign_to_environment = F)
new_df %>%
  janitor::clean_names() %>%
  readr::write_csv("Data/latest_season.csv")
```

```{r echo = F}
new_df <- read_csv(here::here("Data/latest_season.csv"))
```


I made this graph for fun using the wonderful `gtextras` package by Tom Mock and just want to include it in the post now.

```{r}
top5_fg <- new_df %>%
  group_by(slug_position) %>%
  arrange(desc(pct_fg)) %>%
  slice_head(n = 5) %>%
  dplyr::filter(slug_position %in% c("C", "SF", "PF", "PG", "SG")) %>%
  transmute(Position = slug_position,
            url_player_headshot,
            Player = name_player,
            `% FG` = pct_fg, 
            `% EFG` = pct_efg, 
            `% TS` = pct_true_shooting) %>%
  gt::gt() %>%
  fmt_percent(
    columns = c(`% FG`, `% EFG`, `% TS`)
  ) %>%
  gtExtras::gt_color_rows(c(`% FG`, `% EFG`, `% TS`), palette = "rcartocolor::Mint") %>%
  text_transform(
    locations = cells_body(columns = url_player_headshot),
    fn = function(x) {
      web_image(
        url = x,
        height = as.numeric(50)
      )
    }
  ) %>%
  cols_label(
    url_player_headshot = " "
  ) %>%
  tab_header("Top 5 Field Goal % in Each Position in the 2021-2022 Season") %>%
  DGThemes::gt_theme_duncan() %>%
  suppressWarnings()

top5_fg

# top5_fg %>%
#   gtsave(here::here("images/top5.png"))
```

## Analysis

Finally, some machine learning, first we filter down to the relevant variables and start off by using the relevant variables for our model to predict maximum fantasy points.

```{r}
normal_df <- df %>%
  mutate(id = slugPlayerSeason) %>%
  transmute(count_games = countGames,
            count_games_started = countGamesStarted,
            pct_fg = pctFG,
            pct_fg3 = pctFG3,
            pct_fg2 = pctFG2,
            pct_efg = pctEFG,
            pct_ft = pctFT,
            minutes = minutesTotals,
            fgm_total = fgmTotals,
            fga_total = fgaTotals,
            fg3m_totals = fg3mTotals,
            fg3a_totals = fg3aTotals,
            fg2m_totals = fg2mTotals,
            fg2a_totals = fg2aTotals,
            ftm_totals = ftmTotals,
            fta_totals = ftaTotals,
            orb_totals = orbTotals,
            drb_totals = drbTotals,
            trb_totals = trbTotals,
            ast_totals = astTotals,
            stl_totals = stlTotals,
            blk_totals = blkTotals,
            tov_totals = tovTotals,
            pf_totals = pfTotals,
            pts_totals = ptsTotals,
            fantasy_points = fantasy_points)
```

## Data Exploration

Lots of statistics here idk really.

```{r}
normal_df %>%
  pivot_longer(count_games:pts_totals, names_to = "stat", values_to = "value") %>%
  ggplot(aes(value, fantasy_points, color = stat)) +
  geom_point(alpha = 0.6) +
  ggtitle("Fantasy Points by Statistic each Season from the 1999-2000 to 2020-2021 NBA Season") +
  labs(y = "Fantasy Points", x = "") +
  paletteer::scale_color_paletteer_d(palette = "dichromat::SteppedSequential_5") +
  facet_wrap( ~ stat, ncol = 5, scales = "free_x") +
  theme(legend.position = "none")

ggsave(here::here("images/eda_points_stat.png"), width = 10, height = 10)
```

## Building a Model

We start by loading the tidymodels metapackage, and splitting the data into training and testing sets.

```{r}
# Later add a part to strata by 3 point era?
library(tidymodels)

set.seed(2021)
nba_split <- initial_split(normal_df)
nba_train <- training(nba_split)
nba_test <- testing(nba_split)
```

An XGBoost model is based on trees, so we don't have to do anymore preprocessing than has already been done on the data, we can go straight into model specification and hyperparameter tuning.

```{r}
xgb_spec <- boost_tree(
  trees = 1000,
  tree_depth = tune(), min_n = tune(), 
  loss_reduction = tune(),                     ## first three: model complexity
  sample_size = tune(), mtry = tune(),         ## randomness
  learn_rate = tune()                          ## step size
) %>%
  set_engine("xgboost") %>%
  set_mode("regression")

xgb_spec
```

Thats a lot right there so lets create a space-filling design to cover the hyperparameter space as efficiently as possible.

```{r}
xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(),
  finalize(mtry(), nba_train),
  learn_rate(),
  size = 30
)

xgb_grid
```

Note that `mtry()` had to be treated differently because it depends on the number of predictors in the data.

The model specification in the workflow must be put in for convenience. since theres no preprocessing necessary here we can go straight to `add_formula()` as our data preprocessor.

```{r}
xgb_wf <- workflow() %>%
  add_formula(fantasy_points ~ .) %>%
  add_model(xgb_spec)

xgb_wf
```

Next we create the cross-validation resamples for tuning the model.

```{r}
set.seed(2021)
nba_folds <- vfold_cv(nba_train)

nba_folds
```

Now we get ready for the big task, tuning the grid with `tune_grid()` using the workflow, resamples, and grid of paramaters to try. We use `control_grid(save_pred = T)` to explore predictions afterwards.

```{r echo = F}
xgb_res <- read_rds(here::here("models/xgb_res.rds"))
```


```{r eval = F}
doParallel::registerDoParallel()

set.seed(2021)
xgb_res <- tune_grid(
  xgb_wf,
  resamples = nba_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = T)
)
```

```{r}
xgb_res
```

## Exploring Results

It's easy to explore all metrics for the models with `collect_metrics()`!

```{r}
collect_metrics(xgb_res)
```

We can visualize these easily as well to understand the results

```{r}
xgb_res %>%
  collect_metrics() %>%
  filter(.metric == "rsq") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry:sample_size,
               values_to = "value",
               names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "R Squared")
```

And let tidymodels select the best performing set of paramaters for us with `show_best()`, its pretty convenient that the best rmse is also the best r-squared one.

```{r}
show_best(xgb_res, "rmse");show_best(xgb_res, "rsq")
```

Or the best performing model overall

```{r}
best_rsq <- select_best(xgb_res, "rsq")
best_rsq
```

Now to finalize the tuneable workflow with the paramater values we have

```{r}
final_xgb <- finalize_workflow(
  xgb_wf,
  best_rsq
)

final_xgb
```

Instead of `tune()` placeholders, we can use real values for the models hyperparameters.

Which of the parameters is most important for variable importance?

```{r}
library(vip)

final_xgb %>%
  fit(data = nba_train) %>%
  pull_workflow_fit() %>%
  vip(geom = "point")
```

Looks like minutes has the most importance, and then points, and then field goal makes.

And to save it with `xgboost::xgb.save()`! It can be reloaded later with `xgboost::xgb.load()`.


```{r}
final_xgb %>%
  fit(data = nba_train) %>%
  extract_fit_engine() %>%
  xgboost::xgb.save(., here::here("models/normal_model"))
```

Finally lets use `last_fit()` to fit the model one last time on the training data and *evaluate* our model one last time on the testing set. Notice that this is the first time we have used the testing data during this whole modeling analysis.

```{r}
final_res <- last_fit(final_xgb, nba_split)

collect_metrics(final_res)
```

Our results indicate some overfitting we will find out. Here we use an ROC curve for the testing set. Remember that the ROC curve shows the true positive rate vs the false positive rate

```{r}
final_res %>%
  collect_predictions() %>%
  ggplot(aes(fantasy_points, .pred, color = id)) +
  geom_abline(lty = 2, color = "gray80", size = 1.5) +
  geom_point(alpha = 0.2) +
  labs(x = "Truth", y = "Predicted Value",
       color = NULL,
       title = "Predicted and True Values for Fantasy Points")
```

